# **RÃ‰SUMÃ‰ DU PROJET ETL OBRAIL EUROPE**

## âœ… **PHASE 1 : EXTRACTION - DÃ‰JÃ€ FAIT**
- **GTFS France** : SNCF - trains de jour ğŸ‡«ğŸ‡·
- **GTFS Suisse** : SBB/CFF - transports publics ğŸ‡¨ğŸ‡­  
- **GTFS Allemagne** : Deutsche Bahn - trains allemands ğŸ‡©ğŸ‡ª
- **Eurostat** : Statistiques trafic/passagers ferroviaires europÃ©ens ğŸ“Š
- **Back-on-Track** : Trains de nuit en Europe ğŸŒ™
- **Ã‰missions CO2** : DonnÃ©es environnementales Eurostat ğŸŒ

## âœ… **PHASE 2 : TRANSFORMATION - DÃ‰JÃ€ FAIT**
### **Nettoyage par source :**
- **Back-on-Track** : Noms de villes, trains de nuit, opÃ©rateurs
- **Eurostat** : DonnÃ©es pivotÃ©es, filtrage >2010, remplissage moyennes
- **Ã‰missions CO2** : Filtrage CO2 uniquement, normalisation pays
- **GTFS (FR/CH/DE)** : Agences, routes, arrÃªts, voyages

### **Enrichissement et structuration :**
- **ModÃ¨le en Ã©toile** crÃ©Ã© pour data warehouse â­
- **Tables dimensionnelles** : Pays, AnnÃ©es, OpÃ©rateurs
- **Table de faits** : Trajets (trains de nuit)
- **MÃ©triques dashboard** : CO2/passager, trafic par pays

### **QualitÃ© et conformitÃ© :**
- **Filtrage temporel** : DonnÃ©es depuis 2010 uniquement
- **Remplissage intelligent** : Moyennes par pays au lieu de suppression
- **Rapports RGPD** : TraÃ§abilitÃ© complÃ¨te des transformations
- **Documentation JSON** : Rapports qualitÃ© automatiques

## ğŸ“ **STRUCTURE DES DONNÃ‰ES ACTUELLE**
```
data/
â”œâ”€â”€ raw/          # â† DonnÃ©es brutes extraites (6 sources)
â”œâ”€â”€ processed/    # â† DonnÃ©es nettoyÃ©es par source
â””â”€â”€ warehouse/    # â† Data warehouse prÃªt pour BDD
    â”œâ”€â”€ facts_trips.csv          # Trajets de nuit
    â”œâ”€â”€ dim_countries.csv        # Pays europÃ©ens  
    â”œâ”€â”€ dim_years.csv           # AnnÃ©es 2010-2024
    â”œâ”€â”€ dim_operators.csv       # OpÃ©rateurs ferroviaires
    â”œâ”€â”€ dashboard_metrics.csv   # MÃ©triques pour visualisation
    â”œâ”€â”€ quality_reports.json    # Rapports qualitÃ©
    â””â”€â”€ rgpd_traceability_report.json  # ConformitÃ© RGPD
```

## ğŸš€ **PIPELINE FONCTIONNEL**
- **Script principal** : `etl/main_etl.py` (menu interactif)
- **Option 1** : Pipeline complet (extraction + transformation)
- **Option 2** : Extraction seule
- **Option 3** : Transformation seule  
- **Option 4** : Ã‰tat des donnÃ©es

## ğŸ”„ **LOGICIEL UTILISÃ‰**
- **Python** : pandas, numpy, requests
- **Formats** : CSV, JSON, APIs REST
- **Architecture** : ModÃ¨le en Ã©toile pour data warehouse

## ğŸ¯ **PROCHAINES Ã‰TAPES Ã€ FAIRE**
1. **PHASE 3 : CHARGEMENT** dans PostgreSQL
2. **API REST** pour exposer les donnÃ©es
3. **Dashboard** de visualisation
4. **Dockerisation** du projet
5. **Documentation** technique complÃ¨te

## ğŸ’¡ **VALEUR AJOUTÃ‰E DÃ‰JÃ€ CRÃ‰Ã‰E**
- âœ… **Centralisation** : 6 sources hÃ©tÃ©rogÃ¨nes â†’ 1 data warehouse
- âœ… **QualitÃ©** : Nettoyage, validation, mÃ©triques qualitÃ©
- âœ… **Analyse prÃªte** : DonnÃ©es structurÃ©es pour comparaison jour/nuit
- âœ… **ConformitÃ©** : RGPD, traÃ§abilitÃ©, documentation
- âœ… **Automatisation** : Pipeline ETL reproductible

**Ã‰tat actuel** : âœ… **TRANSFORMATION TERMINÃ‰E**


##  **PHASE 3 : CHARGEMENT DES DONNÃ‰ES (Ã€ RÃ‰ALISER)**

### ğŸ¯ **Objectif**

Charger les donnÃ©es du **data warehouse** dans une **base de donnÃ©es PostgreSQL** en respectant une **architecture en Ã©toile**, afin de permettre des requÃªtes analytiques performantes et lâ€™exposition future via une API et un dashboard.

### **Ã‰tapes Ã  rÃ©aliser**

1. **CrÃ©ation de lâ€™architecture de la base de donnÃ©es**

   * DÃ©finir le **schÃ©ma en Ã©toile** (tables de faits et dimensions) dans PostgreSQL
   * ImplÃ©menter cette structure dans le fichier :

     ```
     sql/01_init.sql
     ```
   * CrÃ©ation des tables :
     * tables techniques si nÃ©cessaire (logs, mÃ©tadonnÃ©es)

2. **Scripts de chargement (Load)**

   * CrÃ©er des scripts Python dans le dossier :

     ```
     etl/load/
     ```
   * Ces scripts devront :

     * Lire les fichiers du dossier `data/warehouse/`
     * Se connecter Ã  PostgreSQL
     * InsÃ©rer les donnÃ©es dans les tables correspondantes
     * GÃ©rer les clÃ©s primaires / Ã©trangÃ¨res
     * Ã‰viter les doublons (upsert si nÃ©cessaire)

3. **Dockerisation et dÃ©marrage de lâ€™environnement**

   * Sâ€™assurer que **Docker est lancÃ©**
   * DÃ©marrer les services avec la commande :

     ```bash
     docker compose up -d
     ```
   * En cas de problÃ¨me :

     * VÃ©rifier les logs Docker
     * VÃ©rifier la configuration du fichier :

       ```
       docker-compose.yaml
       ```

4. **Tests et validation**

   * Tester la connexion Ã  PostgreSQL via le terminal
   * ExÃ©cuter des **requÃªtes SQL de vÃ©rification**, par exemple :

     * Nombre de trajets chargÃ©s
     * Jointures entre table de faits et dimensions
     * VÃ©rification de la cohÃ©rence des donnÃ©es
   * Confirmer que lâ€™ensemble du pipeline **ETL â†’ BDD** fonctionne correctement

### âœ… **RÃ©sultat attendu**

* DonnÃ©es du data warehouse correctement chargÃ©es dans PostgreSQL
* Architecture en Ã©toile fonctionnelle
* Base prÃªte pour :

  * API REST
  * Dashboard de visualisation
  * Analyses avancÃ©es et comparaisons europÃ©ennes

