# =========================================================
# ETL/transform/audit_data_loss.py
# Audit des pertes de donnÃ©es â€“ ObRail Europe
# =========================================================

"""
Audit des pertes de donnÃ©es entre les diffÃ©rentes Ã©tapes du pipeline ETL.
Ce script compare les donnÃ©es entre RAW, PROCESSED et WAREHOUSE.
"""

import pandas as pd
from pathlib import Path

RAW_DIR = Path("data/raw")
PROCESSED_DIR = Path("data/processed")
WAREHOUSE_DIR = Path("data/warehouse")

def audit_back_on_track():
    """Audit des donnÃ©es Back-on-Track."""
    print("\n" + "="*60)
    print("ğŸ” AUDIT BACK-ON-TRACK")
    print("="*60)
    
    # Cities
    try:
        raw_cities = pd.read_csv(RAW_DIR / "back_on_track" / "view_ontd_cities.csv")
        processed_cities = pd.read_csv(PROCESSED_DIR / "back_on_track" / "view_ontd_cities.csv")
        wh_cities = pd.read_csv(WAREHOUSE_DIR / "cities.csv")
        
        print("\nğŸ“Š Villes:")
        print(f"  RAW: {len(raw_cities)} lignes")
        print(f"  PROCESSED: {len(processed_cities)} lignes")
        print(f"  WAREHOUSE: {len(wh_cities)} lignes")
        print(f"  Pertes RAWâ†’PROCESSED: {len(raw_cities) - len(processed_cities)}")
        print(f"  Pertes PROCESSEDâ†’WAREHOUSE: {len(processed_cities) - len(wh_cities)}")
        print(f"  Pertes totales: {len(raw_cities) - len(wh_cities)}")
        
        # VÃ©rifier les villes sans nom
        missing_names = wh_cities[wh_cities["city_name"].isin(["nan", "NaN", "Inconnu", ""])]
        print(f"  Villes sans nom valide dans warehouse: {len(missing_names)}")
        
        if len(missing_names) > 0:
            print(f"  Exemples de villes problÃ©matiques:")
            print(missing_names.head().to_string())
    
    except FileNotFoundError as e:
        print(f"  âŒ Fichier manquant: {e}")
    
    # Routes
    try:
        raw_routes = pd.read_csv(RAW_DIR / "back_on_track" / "view_ontd_list.csv")
        wh_routes = pd.read_csv(WAREHOUSE_DIR / "routes.csv")
        
        print("\nğŸ“Š Routes:")
        print(f"  RAW: {len(raw_routes)} lignes")
        print(f"  WAREHOUSE: {len(wh_routes)} lignes")
        print(f"  Pertes: {len(raw_routes) - len(wh_routes)}")
        
    except FileNotFoundError as e:
        print(f"  âŒ Fichier manquant: {e}")
    
    # Route Countries
    try:
        wh_route_countries = pd.read_csv(WAREHOUSE_DIR / "route_countries.csv")
        print(f"\nğŸ“Š Associations Route-Pays:")
        print(f"  WAREHOUSE: {len(wh_route_countries)} associations")
        
        # VÃ©rifier la distribution
        route_counts = wh_route_countries["route_id"].value_counts()
        print(f"  Nombre moyen de pays par route: {wh_route_countries.groupby('route_id').size().mean():.2f}")
    
    except FileNotFoundError as e:
        print(f"  âŒ Fichier manquant: {e}")

def audit_gtfs_fr():
    """Audit des donnÃ©es GTFS France."""
    print("\n" + "="*60)
    print("ğŸ” AUDIT GTFS FRANCE")
    print("="*60)
    
    try:
        # Stops
        raw_stops = pd.read_csv(RAW_DIR / "gtfs_fr" / "stops.csv")
        processed_stops = pd.read_csv(PROCESSED_DIR / "gtfs_fr" / "stops_clean.csv")
        
        print("\nğŸ“Š ArrÃªts:")
        print(f"  RAW: {len(raw_stops)} lignes")
        print(f"  PROCESSED: {len(processed_stops)} lignes")
        print(f"  Pertes: {len(raw_stops) - len(processed_stops)}")
        print(f"  % conservÃ©: {(len(processed_stops)/len(raw_stops))*100:.1f}%")
        
        # Routes
        raw_routes = pd.read_csv(RAW_DIR / "gtfs_fr" / "routes.csv")
        processed_routes = pd.read_csv(PROCESSED_DIR / "gtfs_fr" / "routes_clean.csv")
        
        print("\nğŸ“Š Lignes:")
        print(f"  RAW: {len(raw_routes)} lignes")
        print(f"  PROCESSED: {len(processed_routes)} lignes")
        print(f"  Pertes: {len(raw_routes) - len(processed_routes)}")
        print(f"  % conservÃ©: {(len(processed_routes)/len(raw_routes))*100:.1f}%")
        
    except FileNotFoundError as e:
        print(f"  âŒ Fichier manquant: {e}")

def audit_eurostat():
    """Audit des donnÃ©es Eurostat."""
    print("\n" + "="*60)
    print("ğŸ” AUDIT EUROSTAT")
    print("="*60)
    
    eurostat_files = list((RAW_DIR / "eurostat").glob("*.csv"))
    
    if not eurostat_files:
        print("  â„¹ï¸ Aucun fichier Eurostat trouvÃ©")
        return
    
    for file in eurostat_files:
        try:
            raw_df = pd.read_csv(file)
            processed_file = PROCESSED_DIR / "eurostat" / file.name.replace(".csv", "_clean.csv")
            
            if processed_file.exists():
                processed_df = pd.read_csv(processed_file)
                print(f"\nğŸ“Š {file.name}:")
                print(f"  RAW: {len(raw_df)} lignes, {len(raw_df.columns)} colonnes")
                
                # Identifier les colonnes d'annÃ©es dans RAW
                year_cols_raw = [c for c in raw_df.columns if c.isdigit()]
                if year_cols_raw:
                    min_year_raw = min(map(int, year_cols_raw))
                    max_year_raw = max(map(int, year_cols_raw))
                    print(f"     PÃ©riode RAW: {min_year_raw} - {max_year_raw}")
                
                print(f"  PROCESSED: {len(processed_df)} lignes, {len(processed_df.columns)} colonnes")
                
                # Identifier les annÃ©es dans PROCESSED
                if 'year' in processed_df.columns:
                    min_year_proc = processed_df['year'].min()
                    max_year_proc = processed_df['year'].max()
                    print(f"     PÃ©riode PROCESSED: {int(min_year_proc)} - {int(max_year_proc)}")
                    print(f"     Filtre â‰¥2013: âœ… appliquÃ©")
                
                # Calculer le % de donnÃ©es conservÃ©es aprÃ¨s 2012
                if year_cols_raw:
                    total_years_raw = len(year_cols_raw)
                    years_after_2012 = len([y for y in year_cols_raw if int(y) >= 2013])
                    if total_years_raw > 0:
                        print(f"     AnnÃ©es conservÃ©es: {years_after_2012}/{total_years_raw} ({years_after_2012/total_years_raw*100:.1f}%)")
            
            else:
                print(f"\nğŸ“Š {file.name}:")
                print(f"  RAW: {len(raw_df)} lignes, {len(raw_df.columns)} colonnes")
                print(f"  PROCESSED: âŒ Fichier non gÃ©nÃ©rÃ©")
        
        except Exception as e:
            print(f"  âŒ Erreur avec {file.name}: {e}")

def audit_warehouse():
    """Audit complet du warehouse."""
    print("\n" + "="*60)
    print("ğŸ¢ AUDIT WAREHOUSE COMPLET")
    print("="*60)
    
    if not WAREHOUSE_DIR.exists():
        print("  âŒ Dossier warehouse introuvable")
        return
    
    warehouse_files = list(WAREHOUSE_DIR.glob("*.csv"))
    
    if not warehouse_files:
        print("  â„¹ï¸ Aucun fichier dans le warehouse")
        return
    
    print(f"\nğŸ“ Fichiers dans le warehouse ({len(warehouse_files)}):")
    
    total_rows = 0
    for file in warehouse_files:
        try:
            df = pd.read_csv(file)
            rows = len(df)
            cols = len(df.columns)
            total_rows += rows
            print(f"  ğŸ“„ {file.name}: {rows} lignes Ã— {cols} colonnes")
            
            # Afficher un aperÃ§u pour les petites tables
            if rows <= 50:
                print(f"    AperÃ§u:")
                print(df.head().to_string())
                print()
        
        except Exception as e:
            print(f"  âŒ Erreur avec {file.name}: {e}")
    
    print(f"\nğŸ“ˆ Total des donnÃ©es dans le warehouse: {total_rows} lignes")

def main():
    """Fonction principale d'audit."""
    print("\n" + "="*60)
    print("ğŸ” AUDIT COMPLET DES PERTES DE DONNÃ‰ES")
    print("="*60)
    
    audit_back_on_track()
    audit_gtfs_fr()
    audit_eurostat()
    audit_warehouse()
    
    print("\n" + "="*60)
    print("âœ… AUDIT TERMINÃ‰")
    print("="*60)

if __name__ == "__main__":
    main()