"""
Script principal de transformation orchestrant tous les processus
"""
import logging
from pathlib import Path
import json
from datetime import datetime

from .back_on_track import transform_back_on_track
from .eurostat import transform_eurostat
from .emissions import transform_emissions
from .gtfs import transform_all_gtfs
from .enrichment import enrich_and_prepare_for_warehouse

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def main_transform_pipeline():
    """
    Pipeline principal de transformation
    """
    logger.info("üöÄ D√©marrage du pipeline de transformation ETL")
    
    # Configuration des chemins
    BASE_DIR = Path(__file__).parent.parent.parent
    RAW_DIR = BASE_DIR / "data" / "raw"
    PROCESSED_DIR = BASE_DIR / "data" / "processed"
    WAREHOUSE_DIR = BASE_DIR / "data" / "warehouse"
    
    # S'assurer que les r√©pertoires existent
    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)
    WAREHOUSE_DIR.mkdir(parents=True, exist_ok=True)
    
    quality_reports = []
    
    try:
        # 1. Transformation Back on Track
        logger.info("=== Transformation Back on Track ===")
        report1 = transform_back_on_track(str(RAW_DIR), str(PROCESSED_DIR))
        quality_reports.append(report1)
        
        # 2. Transformation Eurostat
        logger.info("=== Transformation Eurostat ===")
        report2 = transform_eurostat(str(RAW_DIR), str(PROCESSED_DIR))
        quality_reports.append(report2)
        
        # 3. Transformation √âmissions
        logger.info("=== Transformation √âmissions CO2 ===")
        report3 = transform_emissions(str(RAW_DIR), str(PROCESSED_DIR))
        quality_reports.append(report3)
        
        # 4. Transformation GTFS
        logger.info("=== Transformation GTFS ===")
        reports_gtfs = transform_all_gtfs(str(RAW_DIR), str(PROCESSED_DIR))
        quality_reports.extend(reports_gtfs)
        
        # 5. Enrichissement et pr√©paration pour le data warehouse
        logger.info("=== Enrichissement et pr√©paration Data Warehouse ===")
        traceability_report = enrich_and_prepare_for_warehouse(
            str(PROCESSED_DIR), 
            str(WAREHOUSE_DIR)
        )
        
        # 6. Sauvegarder les rapports de qualit√©
        quality_report_path = WAREHOUSE_DIR / "quality_reports.json"
        with open(quality_report_path, 'w') as f:
            json.dump({
                'execution_date': datetime.now().isoformat(),
                'reports': quality_reports,
                'summary': {
                    'total_sources_processed': len(quality_reports),
                    'total_records_estimated': sum(r.get('total_records', 0) for r in quality_reports if r),
                    'success': True
                }
            }, f, indent=2, ensure_ascii=False)
        
        logger.info(f"‚úÖ Pipeline de transformation termin√© avec succ√®s!")
        logger.info(f"üìä Rapports sauvegard√©s dans: {quality_report_path}")
        logger.info(f"üìÅ Data warehouse pr√™t dans: {WAREHOUSE_DIR}")
        
        # Afficher un r√©sum√©
        print("\n" + "="*50)
        print("R√âSUM√â DE LA TRANSFORMATION")
        print("="*50)
        for report in quality_reports:
            if report:
                source = report.get('source', 'Inconnu')
                records = report.get('total_records', report.get('passengers_records', 0))
                print(f"‚Ä¢ {source.upper():<20} : {records:,} enregistrements trait√©s")
        
        print(f"‚Ä¢ {'TOTAL':<20} : {sum(r.get('total_records', 0) for r in quality_reports if r):,} enregistrements")
        print("="*50)
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la transformation: {e}")
        raise

if __name__ == "__main__":
    main_transform_pipeline()