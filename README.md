# Projet MSPR â€“ ObRail Europe

## ğŸ¯ Contexte du projet

Ce projet sâ€™inscrit dans le cadre de la **MSPR â€“ Bloc E6.1** du programme *DÃ©veloppeur en Intelligence Artificielle et Data Science (RNCP36581)*.

Le client fictif **ObRail Europe** est un observatoire indÃ©pendant spÃ©cialisÃ© dans lâ€™analyse des dessertes ferroviaires europÃ©ennes (trains de jour et trains de nuit). Lâ€™objectif est de mettre en place **un processus ETL automatisÃ©**, fiable et reproductible, permettant de centraliser des donnÃ©es ferroviaires hÃ©tÃ©rogÃ¨nes afin de les exploiter via **une base de donnÃ©es, une API REST et un tableau de bord analytique**.

---

## ğŸ§  Objectifs principaux

* Centraliser des donnÃ©es issues de plusieurs sources open data europÃ©ennes
* Nettoyer, harmoniser et fiabiliser les donnÃ©es
* Concevoir un **entrepÃ´t de donnÃ©es relationnel**
* Exposer les donnÃ©es via une **API REST documentÃ©e**
* Proposer un **dashboard de suivi et dâ€™analyse**
* Respecter les principes RGPD, de traÃ§abilitÃ© et de documentation

---

## ğŸ”— Sources de donnÃ©es utilisÃ©es

* **transport.data.gouv.fr** â€“ DonnÃ©es GTFS France   "https://eu.ftp.opendatasoft.com/sncf/plandata/Export_OpenData_SNCF_GTFS_NewTripId.zip"

* **mobilitydatabase.org** â€“ GTFS Europe
* **back-on-track.eu** â€“ DonnÃ©es sur les trains de nuit europÃ©ens  "analiser mais rejeter"
* **transit.land** â€“ API internationale de transport  "https://transit.land/api/v2/rest/feeds"
* **Eurostat** â€“ DonnÃ©es environnementales et pays   "https://ec.europa.eu/eurostat/api/dissemination/files"

---

## ğŸ› ï¸ Technologies utilisÃ©es

* **Langage principal** : Python
* **ETL & Data processing** : pandas, requests
* **Base de donnÃ©es** : PostgreSQL / SQL
* **API REST** : FastAPI (ou Node.js possible)
* **Dashboard** : Streamlit
* **Conteneurisation** : Docker & Docker Compose

---

## ğŸ‘¥ Organisation de lâ€™Ã©quipe

### Djamil

* Recherche et sÃ©lection des sources de donnÃ©es
* Scripts dâ€™extraction automatisÃ©e
* Mise en place de lâ€™architecture Docker

### Nafissa
    
* Nettoyage et prÃ©paration des donnÃ©es
* Construction de la base de donnÃ©es
* Chargement des donnÃ©es (Load du ETL)

### Mariam

* Transformation des donnÃ©es
* Conception du **ModÃ¨le Conceptuel de DonnÃ©es (MCD)**
* CrÃ©ation du **ModÃ¨le Physique de DonnÃ©es (MPD)**

### Zeinab
* a remplir

### Travail collectif

* DÃ©veloppement de lâ€™API REST
* Conception du dashboard
* DÃ©finition et suivi des KPI
* Documentation et soutenance

---

## ğŸ“‚ Arborescence du projet

```
project-root/
â”‚
â”œâ”€â”€ docker-compose.yml        # Orchestration globale des services
â”œâ”€â”€ .env                     # Variables dâ€™environnement
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # DonnÃ©es brutes (non modifiÃ©es)
â”‚   â”œâ”€â”€ processed/           # DonnÃ©es nettoyÃ©es et transformÃ©es
â”‚   â””â”€â”€ warehouse/           # DonnÃ©es finales prÃªtes pour la BDD
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract/             # Scripts dâ€™extraction des sources
â”‚   â”œâ”€â”€ transform/           # Nettoyage & harmonisation
â”‚   â”œâ”€â”€ load/                # Chargement PostgreSQL
â”‚   â””â”€â”€ main_etl.py          # Pipeline ETL complet
â”‚
â”œâ”€â”€ platform/
â”‚   â”œâ”€â”€ server/              # API REST
â”‚   â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ database.py
â”‚   â”‚   â”‚   â”œâ”€â”€ models.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dependencies.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â””â”€â”€ front/               # Dashboard Streamlit
â”‚       â”œâ”€â”€ app.py
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ 01_init.sql           # CrÃ©ation des tables PostgreSQL
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.png
    â”œâ”€â”€ mcd.png
    â”œâ”€â”€ mpd.png
    â””â”€â”€ rapport_technique.md
```

---

## ğŸ”„ Fonctionnement global du projet (Pipeline complet)

Le projet suit strictement la chaÃ®ne de traitement suivante :

### **Source â†’ Extraction â†’ Affichage brut â†’ Nettoyage â†’ Harmonisation â†’ Stockage**

### 1ï¸âƒ£ Choix des donnÃ©es

Les sources open data sont sÃ©lectionnÃ©es selon leur pertinence (couverture gÃ©ographique, fiabilitÃ©, format).

### 2ï¸âƒ£ Extraction

Les scripts du dossier `etl/extract/` rÃ©cupÃ¨rent les donnÃ©es :

* TÃ©lÃ©chargement de fichiers GTFS (ZIP â†’ CSV)
* Appels API (Transit.land, Eurostat)
* Lecture de fichiers CSV / Excel

Les donnÃ©es sont stockÃ©es dans `data/raw/`.

### 3ï¸âƒ£ Affichage brut

Chaque extraction affiche :

* aperÃ§u des donnÃ©es (`head()`)
* schÃ©ma (`info()`)
* valeurs manquantes

Cette Ã©tape permet de comprendre la structure avant traitement.

### 4ï¸âƒ£ Nettoyage

Dans `etl/transform/` :

* suppression des doublons
* gestion des valeurs manquantes
* correction des formats (dates, pays, codes gares)

Les donnÃ©es passent dans `data/processed/`.

### 5ï¸âƒ£ Harmonisation

* Normalisation des rÃ©fÃ©rentiels (pays, gares, opÃ©rateurs)
* Fusion des sources hÃ©tÃ©rogÃ¨nes
* SÃ©paration trains de jour / trains de nuit

### 6ï¸âƒ£ Chargement

Le chargement dans PostgreSQL est organisÃ© en plusieurs scripts spÃ©cialisÃ©s 

**Structure des scripts de chargement** (`etl/load/`) :
* database.py # Gestion de la connexion PostgreSQL
* main_load.py # Orchestrateur principal du chargement
* load_countries.py # Table dim_countries
* load_years.py # Table dim_years
* load_operators.py # Table dim_operators
* load_night_trains.py # Table facts_night_trains
* load_country_stats.py # Table facts_country_stats

---

## ğŸ—„ï¸ Base de donnÃ©es

* SGBD : PostgreSQL
* ModÃ©lisation : MCD + MPD (dans `/docs`)
* Tables principales : dim_countries, dim_years, dim_operators, facts_night_trains, facts_country_stats

---

## ğŸŒ API REST

* DÃ©veloppÃ©e avec FastAPI
* Expose les donnÃ©es via des endpoints REST
* Filtres : type de train, pays, opÃ©rateur, villes
* Documentation automatique (Swagger)

---

## ğŸ“Š Dashboard

* RÃ©alisÃ© avec Streamlit
* Indicateurs clÃ©s :

  * Nombre de trajets jour / nuit
  * RÃ©partition par pays
  * Taux de complÃ©tude des donnÃ©es
  * Volume de donnÃ©es par source

---

## ğŸš€ Lancer le projet

### PrÃ©requis

* Docker
* Docker Compose

### DÃ©marrage

```bash
docker-compose up --build
```

### AccÃ¨s aux services

* PostgreSQL : `localhost:5432`
* API REST : `http://localhost:8000/docs`
* Dashboard : `http://localhost:8501`

---

## ğŸ“Œ TÃ¢ches couvertes par le projet

1. Choix des donnÃ©es
2. Mise en place Docker
3. Conception MCD / MPD
4. ETL complet
5. Base de donnÃ©es
6. API REST
7. Dashboard
8. Documentation technique

---

## ğŸ Conclusion

Ce projet fournit Ã  ObRail Europe un **entrepÃ´t de donnÃ©es fiable, documentÃ© et Ã©volutif**, prÃªt Ã  Ãªtre exploitÃ© pour des analyses avancÃ©es, des modÃ¨les dâ€™IA et des dÃ©cisions stratÃ©giques liÃ©es Ã  la mobilitÃ© durable en Europe.
