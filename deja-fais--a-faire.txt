Les donnÃ©es rÃ©cupÃ©rÃ©es dans le cadre du MSPR :
data/raw/
â”œâ”€â”€ gtfs_fr/
â”œâ”€â”€ eurostat/
â””â”€â”€ back_on_track/


-Pourquoi ces donnÃ©es :
GTFS France         â†’     horaires, gares, trains de jour
Back-on-Track       â†’     itinÃ©raires des trains de nuit europÃ©ens
Eurostat            â†’     statistiques trafic/fret par pays et type de train
Elles couvrent tous les besoins : jour/nuit, micro/macro donnÃ©es, historique europÃ©en.

-Comment elles se complÃ¨tent :
GTFS + Back-on-Track          â†’   rÃ©seau complet Europe jour/nuit
GTFS + Eurostat               â†’   micro-donnÃ©es + statistiques
Back-on-Track + Eurostat      â†’   trains de nuit vs trafic global

-Extrait des donnÃ©es :
Un extrait de chaque dataset est disponible dans le fichier extrais_dataset.txt, tu peux verifier et directement copier a GPT pour qu'il t'aide

-But du projet :
Nettoyer et centraliser des donnÃ©es ferroviaires europÃ©ennes pour crÃ©er un entrepÃ´t exploitable
permettant dâ€™analyser les trains de jour et de nuit et de mesurer leur rÃ´le dans une mobilitÃ© durable. 
On peut Ã©galement les comparer avec les transports aÃ©riens pour plus de pertinence.

-Fonctions dâ€™extraction :
Les scripts se trouvent dans "etl/extract/...." Pour rÃ©cupÃ©rer les datasets
il suffit de lancer le main situÃ© dans "etl/main_etl.py": tout sera rÃ©cupÃ©rÃ© automatiquement.


pour l'instant pas de docker, c plus tard que je men occuperait
lier les script de nettoyage et de transformation a "main_etl.py" pour tout automatiser, comme j'ai fais pour les scriptsd'extraction






---

Nettoyage et prÃ©paration des donnÃ©es 

Objectif

Rendre les donnÃ©es :
- propres
- cohÃ©rentes
- prÃªtes Ã  Ãªtre stockÃ©es en base de donnÃ©es

Sans transformation mÃ©tier.

ğŸ”¹ GTFS France â€“ Nettoyage

ğŸ“‚ Source :
data/raw/gtfs_fr/

ğŸ“‚ Sortie :
data/processed/gtfs_fr/

Fichiers nettoyÃ©s :
- agency_clean.csv â†’ opÃ©rateurs ferroviaires
- routes_clean.csv â†’ lignes ferroviaires (rail uniquement)
- stops_clean.csv â†’ gares (coordonnÃ©es, nom)

Pourquoi seulement ces fichiers ?
Les fichiers GTFS plus dÃ©taillÃ©s (trips, stop_times, calendar_dates) dÃ©crivent les horaires et Ã©vÃ©nements : ils ne sont pas nÃ©cessaires pour construire
l'entrepÃ´t cible dans la portÃ©e de ce projet.

Travail effectuÃ© :
- lecture des CSV bruts,
- suppression des doublons,
- filtrage (ex. route_type == 2 pour ne conserver que le rail),
- sÃ©lection des colonnes utiles et Ã©criture en `data/processed/gtfs_fr/`.

Remarque sur les annÃ©es antÃ©rieures Ã  2010 :
Nous avons choisi de **conserver** les donnÃ©es antÃ©rieures Ã  2010 afin de prÃ©server l'historique pour les analyses longue durÃ©e (tendances, comparaisons temporelles) et garantir la reproductibilitÃ©. Si l'Ã©quipe souhaite rÃ©duire l'Ã©chelle temporelle, on pourra ajouter un filtre optionnel (`min_year=2010`) dans le pipeline, mais il ne s'appliquera pas par dÃ©faut.


ğŸ”¹ Eurostat â€“ Nettoyage

ğŸ“‚ Source :
data/raw/eurostat/

ğŸ“‚ Sortie :
data/processed/eurostat/

Fichiers produits :
- rail_passengers_clean.csv
- rail_traffic_clean.csv

Travail effectuÃ© :
- remplacement des symboles manquants par `pd.NA` (ex. ":"),
- mise Ã  plat (wide â†’ long) pour obtenir `year` / `value`,
- conversion des types (`year` int, `value` numÃ©rique),
- suppression des doublons mais conservation des valeurs manquantes (pas de suppression de pays/annÃ©es).

Remarque : Eurostat est dÃ©jÃ  normalisÃ©, il n'est pas nÃ©cessaire de l'harmoniser davantage.


ğŸ”¹ Back-on-Track â€“ Nettoyage & harmonisation

ğŸ“‚ Sortie intermÃ©diaire :
data/processed/back_on_track/

ğŸ“‚ Sortie finale :
data/warehouse/

Pourquoi harmoniser ?
Les donnÃ©es Back-on-Track mÃ©langent formats et entitÃ©s (routes, villes, opÃ©rateurs, pays). Il a fallu :
- nettoyer (trim(suppression des espaces en dÃ©but et fin d'une chaÃ®ne (et parfois normalisation des espaces intermÃ©diaires)), suppression lignes vides),
- extraire et normaliser opÃ©rateurs (MAJ + strip),
- dÃ©couper les listes de pays, puis `explode`(dÃ©velopper une liste en plusieurs lignes(une ligne par Ã©lÃ©ment), en dupliquant le reste des colonnes) pour obtenir une table route â†” pays,
- normaliser et dÃ©doublonner les villes, produire `cities.csv`.

Fichiers gÃ©nÃ©rÃ©s importants :
- routes.csv (table des liaisons)
- route_countries.csv (association route_id â†” country_code)
- cities.csv (city_id, city_name, country_code)
- operators.csv (operator_name)
- countries.csv (country_code)

---

ğŸ—‚ï¸ RÃ´le des dossiers ajoutÃ©s

Dossier	RÃ´le
etl/transform/	scripts de nettoyage et prÃ©paration
data/processed/	donnÃ©es nettoyÃ©es par source (intermÃ©diaires)
data/warehouse/	donnÃ©es finales prÃªtes pour la base (Il manque les donnÃ©es de GTFS et de Eurostat qui seront ajoutÃ©es pour la partie chargement plus tard.)
etl/main_etl.py	pipeline principal (extraction + nettoyage + harmonisation)


Il suffit de lancer le script "etl/main_etl.py" Ã  la racine pour exÃ©cuter les scripts par rapport au nettoyage et Ã  la transformation.






ğŸ”œ Ã‰tapes suivantes que je dois faire (En attente du MCD/MPD)

- Construction du schÃ©ma PostgreSQL (fichier SQL d'initialisation : `sql/01_init.sql`).
- Ã‰criture des scripts de chargement (Load) pour insÃ©rer les fichiers de `data/warehouse/` dans la base.



IdÃ©es de Tables qu'on pourrait mettre dans la bdd :
- countries (utilisÃ©e par: Eurostat, Back-on-Track, gares)
- operators (vient de agency_clean.csv + Back-on-Track)
- stations (GTFS + Back-on-Track (villes / gares))
- routes (GTFS + Back-on-Track (routes de nuit))
- night_trains (Back-on-Track)
- rail_passengers (Eurostat)
- rail_traffic (Eurostat)
-
-
-
-
...


---
